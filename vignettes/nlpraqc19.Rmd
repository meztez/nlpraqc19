---
title: "Traitement du langage naturel avec R"
author: "Bruno Tremblay"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: false
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Traitement du langage naturel avec R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r global_options, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Survol

L'objectif de cet atelier et de vous présenter différentes méthodes pour intégrer des documents
dans un contexte d'apprentissage machine. Vous devrez ensuite appliquer ces méthodes pour
entraîner un modèle de classification binaire.

À la base, un document doit être transformé en une représentation numérique pour pouvoir être
utilisé dans un algorithme d'entraînement.

La technique utilisée pour effectuer cette transformation a un grand impact sur les résultats.
C'est ce que nous explorerons aujourd'hui.

# Agenda

1. Base
    + Comment importer des documents (tm, rvest, odbc, jsonlite)
    + Un petit mot sur l'encodage
    + Manipuler des chaînes de caractères (grep it like its hot)
2. Transformations
    + Orthographe (hunspell)
    + Stopwords (tm)
    + Stemming (SnowballC)
    + Collocations
    + Contextes négatifs
    + Matrice de fréquence des termes dans un documents
3. Apprentissage machine

# Atelier

## Base

### Comment importer des documents

#### À partir d'une source de données odbc (odbc)

```{r importodbc, eval = FALSE}
library(odbc)
docs <- dbGetQuery(dbConnect(odbc(), "DSN"), "SELECT TEXT FROM COMMENTS")
```

#### À partir d'internet (rvest)

```{r importrvest}
library(rvest)
content <- read_html('https://old.reddit.com/r/Quebec/')
submissions <- content %>% html_nodes('a.title') %>% html_text
links <- content %>% html_nodes('a.title') %>% html_attr("href")
rqclatest <- data.frame("submissions" = submissions, "links" = links)
rqclatest
```

Référence :  
* https://datascienceplus.com/building-a-hacker-news-scraper-with-8-lines-of-r-code-using-rvest-library/

#### À partir de documents pdf ou d'images (tesseract, tm)

```{r importpdf, eval = FALSE}
library(tesseract)
vignette("intro", "tesseract")
```
Références :  
* https://data.library.virginia.edu/reading-pdf-files-into-r-for-text-mining/
* https://medium.com/@CharlesBordet/how-to-extract-and-clean-data-from-pdf-files-in-r-da11964e252e

#### À partir d'un archive de fichiers json

```{r importfiles, eval = FALSE}
library(jsonlite)

archive <- "../extract_json_r_20190412.zip"
unzip(archive, overwrite = TRUE, junkpaths = TRUE, exdir = "../json")
files <- dir("../json", full.names = TRUE, recursive = FALSE)
docs <- sapply(files,
                function(file) {
                  content <- readLines(file, encoding = "latin1", warn = FALSE)
                  content <- iconv(content, from = "ISO8859-1", to = "UTF-8")
                  # content <- gsub("\\\\", "\\\\u005C", content)
                  # content <- gsub("\u0008", "\\\\u0008", content)
                  # content <- gsub("\u0009", "\\\\u0009", content)
                  # content <- gsub("\u000A", "\\\\u000A", content)
                  # content <- gsub("\u000C", "\\\\u000C", content)
                  # content <- gsub("\u000D", "\\\\u000D", content)
                  decoded <- try(fromJSON(content))
                  if (class(decoded) == "try-error") {
                    print(file)
                    return()
                  } else {
                    return(decoded$text-content)
                  }
                })
```

#### À partir d'un data déjà créé pour l'atelier d'aujourd'hui

```{r datatable, echo=FALSE}
library(data.table)
```

```{r importdt}
utils::data(collisions, package = "nlpraqc19")
docs <- nlpraqc19::collisions
docs[sample(1:nrow(docs), 5)]
```

### Un petit mot sur l'encodage

Si vous voyez apparaître des symboles � inattendus dans vos chaînes de caractères,
c'est probablement parce qu'il y a un problème avec l'encodage.

Pour faire une histoire courte, vos programmes utilisent une table d'encodage pour
déterminer la correspondance entre la représentation binaire et la représentation
symbolique des caractères.

Donc pour s'assurer d'avoir la bonne représentation, il faut effectuer la lecture
ou l'écriture des données avec le bon encodage.

```{r encodagelecture, eval = FALSE}
# Définir l'encodage au niveau d'une connexion
dbConnect(odbc(), encoding = "latin1")

# Convertir l'encodage d'une chaîne de caractères
iconv(text, from = "ISO8859-1", to = "UTF-8")
```

Référence :  
https://kevinushey.github.io/blog/2018/02/21/string-encoding-and-r/

### Manipuler des chaînes de caractères (grep it like its hot)

Il y a quelques outils de base qui sont indispensables à la manipulation
de chaînes de caractères.

#### grep, gsub et expressions régulières

Trouver les 10 premiers commentaires qui contiennent le mot "chat".
```{r regex1}
grep("chat", collisions$QUEST_CICH_COMNT)[1:10]
```

Ça nous donne l'indice mais si on veut la valeur.
```{r regex2}
grep("chat", collisions$QUEST_CICH_COMNT, value = TRUE)[1:10]
```

C'est pas tout à fait ça, on veut le mot chat donc il faut mieux définir notre expression de recherche. Ajoutons l'option d'ignorer minuscule masjucule.

```{r regex3}
grep("\\bchat\\b", collisions$QUEST_CICH_COMNT, value = TRUE, ignore.case = TRUE)[1:10]
```

Les expressions régulières sont très puissantes. Elles permettent de trouver rapidement de l'information. Maîtriser les et dominer les mondes... ou presque.

```{r regex4}
?regex
```

Vous pouvez expérimenter avec différentes expressions pour identifier des cas. C'est comme un où est Charlie.



Trouver quelqu'un qui est rentré dans un arbre.

```{r regex5}
txt <- collisions$QUEST_CICH_COMNT
grep("\\brentr", txt[grep("\\barbre\\b",txt)], value = TRUE, ignore.case = TRUE)[1]
```

Bonjour la police

```{r regex6}
grep("voiture de police|char de police", txt, value = TRUE, ignore.case = TRUE)[1:5]
```

Est-ce que je peux avoir votre code postal?

```{r regex7}
grep("[a-z][0-9][a-z][ -]?[[:digit:]][[:alpha:]][[:digit:]]", txt, value = TRUE, ignore.case = TRUE)[1:5]
```

La fonction gsub sert à effectuer des remplacements. Elle pourrait servir à retirer les informations sensibles des champs textes comme les numéros de téléphones.

```{r regex8}
phonepattern <- "\\b\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})\\b"
findtel <- grep(phonepattern, txt, value = TRUE, ignore.case = TRUE)[1]
data.frame("origine" = findtel, "remplacement" = gsub(phonepattern, "", findtel))
```

Les fonctions gregexpr et regmatches permettent d'extraire les valeurs directement des chaînes de caractères.

```{r regex9}  
parsed <- gregexpr(phonepattern, txt)
matches <- regmatches(txt, parsed)
unlist(unlist(matches))[1:10]
```

#### Autres fonctions utiles

Les fonctions substr, strsplit, paste, paste0, tolower, toupper sont également très pratiques.

```{r stringsubstr}
phrase <- sample(txt, 1)
substr(phrase, max(1, nchar(phrase) - 10), nchar(phrase))
```

```{r stringstrsplit}
strsplit(phrase, "[^[:alnum:]]+")
```

```{r stringpaste}
paste("sors avec", unlist(strsplit(phrase, "[^[:alnum:]]+")))
```

```{r stringpaste0}
paste0(21:69, collapse = ",")
```

```{r stringto}
toupper(phrase)

```

## Transformations

### Orthographes

Le package hunspell permet d'effectuer de valider l'orthographe de textes.

Charger hunspell

```{r hunspell1}  
library(hunspell)
```

Télécharger le dictionnaire français le plus à jour.

```{r}
tempf <- tempfile()
tempd <- tempdir()
download.file("http://grammalecte.net/download/fr/hunspell-french-dictionaries-v6.4.1.zip", tempf)
dicts <- grep("tes\\.aff?|tes\\.dic?", unzip(tempf, list = TRUE)$Name, value = TRUE)
unzip(tempf, files = dicts, overwrite = TRUE, junkpaths = TRUE, exdir = tempd)
dict_fr
```

```{r hunspell3}
custom_words <- c("b2","b3","tp","vh","boul","v\U00e9h","faq","vt","faq20",
                 "veh","mtl","43a","qc","rdp","dir","43ae","cond","domms",
                 "20a","iga","aut","ins","ste","st","blv","domm","pcq","bags",
                 "43e","rp","coord","faq43a","berpa","13c","faq27","str")
dict_fr <- dictionary(paste0(tempd, "\\", grep("\\.dic", dicts, value = TRUE)), add_words = custom_words)
dict_en <- dictionary("en_CA", add_words = custom_words)
```

Maintenant regardons des phrases dans nos données.

```{r hunspell4}  
phrases <- sample(txt, 5)
words <- hunspell_parse(phrases, dict = dict_fr)
correct <- lapply(words, hunspell_check, dict_fr)
sapply(correct, all)
```

Trouvez les mots mal orthographiés ou non reconnus.  

```{r hunspell5}
bad <- lapply(1:length(words),function(x) {words[[x]][!correct[[x]]]})
bad
```

On peut aussi passer par hunspell directement.

```{r hunspell6}  
hunspell(phrases, dict = dict_fr)
```

Qu'est-ce que le dictionaire propose?  

```{r hunspell7}
lapply(bad, function(x) {
  suggest <- hunspell_suggest(x, dict = dict_fr)
  names(suggest) <- x
  suggest
  })
```

Tentative d'autocorrection. Si plus de 50% des mots sont mal orthographiés, on va assumer que c'est anglais. Pas efficace mais honnête.

```{r hunspell8}
autocorrect <- function(txt, dict, alt_dict) {
  parsed <- hunspell_parse(txt, dict = dict)
  checked <- lapply(parsed, hunspell_check, dict = dict)
  suggested <- lapply(1:length(parsed), function(i) {
    if (!all(checked[[i]])) {
      if (sum(!checked[[i]])/length(checked[[i]]) > 0.5) {
        checked[[i]] <- hunspell_check(parsed[[i]], alt_dict)
        dict_suggest <- alt_dict
      } else {
        dict_suggest <- dict
      }
      suggested <- unlist(lapply(hunspell_suggest(parsed[[i]][!checked[[i]]], dict = dict_suggest), `[`, 1))
      parsed[[i]][!checked[[i]]] <- ifelse(is.na(suggested), parsed[[i]][!checked[[i]]], suggested)
    }
    parsed[[i]]
  })
  return(unlist(lapply(suggested, paste, collapse = " ")))
}
```

On essaie  

```{r hunspell9}
autocorrect(phrases, dict_fr, dict_en)
autocorrect("hi, grandma? can u come pyck me up from my rap batttle? it's over. no, i lost. he saw u droop me off & did a prety devastating rhyme about it", dict_fr, dict_en)
```

La réalité de la vie c'est que nos données sont pas propre propre et que la correction de mots sans contexte c'est pas facile.


### Présentation du package text2vec

http://text2vec.org/



